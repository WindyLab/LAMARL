<center>

# LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation

</center>

<div align="center">

[![Code](https://img.shields.io/badge/Code-GitHub-blue.svg)](https://github.com/Guobin-Zhu/MARL-LLM)
[![Video](https://img.shields.io/badge/Video-YouTube-ff0000.svg)](https://youtu.be/HoYk70Ksy8w)

**[Guobin Zhu](https://github.com/Guobin-Zhu)<sup>1,2</sup>, [Rui Zhou]()<sup>1</sup>, [Wenkang Ji]()<sup>2</sup>, [Shiyu Zhao](https://www.westlake.edu.cn/faculty/shiyu-zhao.html)<sup>2</sup>**

<sup>1</sup>Beihang University &nbsp;&nbsp;&nbsp; <sup>2</sup>Westlake University

</div>

---

## 🚀 **Quick Access**

<div align="center">

| Resource | Link |
|:--------:|:----:|
| 💻 **Code** | [GitHub Repository](https://github.com/Guobin-Zhu/MARL-LLM) |
| 🎥 **Video** | [YouTube Demo](https://youtu.be/HoYk70Ksy8w) |

</div>

---

## 🎯 **Method Overview**

<div align="center">
 <img src="images/framework.jpg" alt="LAMARL Framework" width="800">
 <br>
 <em>LAMARL Framework</em>
</div>

---

## 📖 **Abstract**

This paper introduces **LAMARL**, a novel approach that integrates Multi-Agent Reinforcement Learning (MARL) with Large Language Models (LLMs) to enhance sample efficiency and automate function generation for multi-robot cooperative tasks. 

### Key Achievements:
- 🎯 **185.9%** improvement in sample efficiency on average
- 🤖 Fully automated prior policy and reward function generation
- 🔧 **28.5%-67.5%** improvement in LLM output success rates through structured prompting
- ✅ Validated on both simulation and real-world shape assembly tasks

---

<div align="center">

**⭐ Star this repo if you find it useful! ⭐**

</div>